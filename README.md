# ğŸš• Uber ELT Pipeline on Databricks

## ğŸ“‹ Má»¥c tiÃªu (Objective)

Project nÃ y xÃ¢y dá»±ng má»™t **Data Lakehouse hoÃ n chá»‰nh** theo kiáº¿n trÃºc **Medallion (Bronze-Silver-Gold)** cho dá»¯ liá»‡u Uber, táº¡o ra cÃ¡c báº£ng phÃ¢n tÃ­ch (Fact/Dimension tables) sáºµn sÃ ng cho business intelligence vÃ  data analytics.

**Váº¥n Ä‘á» giáº£i quyáº¿t:**
- Ingestion dá»¯ liá»‡u tá»« nhiá»u nguá»“n CSV vÃ o Data Lake
- LÃ m sáº¡ch, deduplication vÃ  upsert dá»¯ liá»‡u theo CDC (Change Data Capture)
- Táº¡o dá»¯ liá»‡u lá»‹ch sá»­ vá»›i SCD Type 2 (Slowly Changing Dimensions)
- Cung cáº¥p báº£ng fact incremental cho phÃ¢n tÃ­ch trips

---

## ğŸ—ï¸ Kiáº¿n trÃºc (Architecture)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Source    â”‚      â”‚    Bronze    â”‚      â”‚    Silver    â”‚      â”‚     Gold     â”‚
â”‚  (Volumes)  â”‚â”€â”€â”€â”€â”€â–¶â”‚ (Delta Lake) â”‚â”€â”€â”€â”€â”€â–¶â”‚ (Delta Lake) â”‚â”€â”€â”€â”€â”€â–¶â”‚ (Delta Lake) â”‚
â”‚             â”‚      â”‚              â”‚      â”‚              â”‚      â”‚              â”‚
â”‚ CSV Files:  â”‚      â”‚ Raw ingested â”‚      â”‚ Cleaned +    â”‚      â”‚ Fact Tables  â”‚
â”‚ - customers â”‚      â”‚ data with    â”‚      â”‚ Deduplicated â”‚      â”‚ - FactTrips  â”‚
â”‚ - trips     â”‚      â”‚ full history â”‚      â”‚ + Upserted   â”‚      â”‚              â”‚
â”‚ - locations â”‚      â”‚              â”‚      â”‚              â”‚      â”‚ Dim Tables   â”‚
â”‚ - payments  â”‚      â”‚              â”‚      â”‚              â”‚      â”‚ (SCD Type 2) â”‚
â”‚ - vehicles  â”‚      â”‚              â”‚      â”‚              â”‚      â”‚ - DimCustomersâ”‚
â”‚ - drivers   â”‚      â”‚              â”‚      â”‚              â”‚      â”‚ - DimDrivers  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ - DimVehicles â”‚
                              â”‚                      â”‚            â”‚ - DimLocationsâ”‚
                              â”‚                      â”‚            â”‚ - DimPayments â”‚
                              â–¼                      â–¼            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        PySpark Streaming      PySpark + MERGE           â”‚
                        (trigger once)         (Upsert logic)            â–¼
                                                                    dbt snapshots
                                                                  + incremental models
```

### Luá»“ng dá»¯ liá»‡u chi tiáº¿t:
1. **Bronze Layer**: PySpark Streaming Ä‘á»c CSV tá»« Volumes â†’ Append vÃ o Delta Tables
2. **Silver Layer**: PySpark transformations (dedup + upsert) â†’ MERGE vÃ o Delta Tables
3. **Gold Layer**: dbt snapshots (SCD Type 2) + incremental models â†’ Tables cho Analytics

---

## ğŸ› ï¸ CÃ´ng nghá»‡ sá»­ dá»¥ng (Tech Stack)

| Component | Technology |
|-----------|-----------|
| **Ná»n táº£ng** | Databricks (Community Edition / Trial) |
| **Data Lake** | Delta Lake |
| **Storage** | Databricks Volumes (Unity Catalog) |
| **Ingestion (Bronze)** | PySpark Streaming vá»›i `trigger(once=True)` |
| **Transformation (Silver)** | PySpark + `DeltaTable.merge()` (Upsert logic) |
| **Transformation (Gold)** | dbt-databricks |
| **MÃ´ hÃ¬nh hÃ³a** | dbt Snapshots (SCD Type 2) + Incremental Models |
| **Orchestration** | Databricks Notebooks |
| **Deduplication** | Window functions vá»›i `row_number()` |

---

## ğŸ“Š Cáº¥u trÃºc Dá»¯ liá»‡u (Schema)

### Catalog Structure:
```
pysparkdbt (catalog)
â”œâ”€â”€ source (schema)
â”‚   â””â”€â”€ source_data (volume)
â”‚       â”œâ”€â”€ customers/
â”‚       â”œâ”€â”€ trips/
â”‚       â”œâ”€â”€ locations/
â”‚       â”œâ”€â”€ payments/
â”‚       â”œâ”€â”€ vehicles/
â”‚       â””â”€â”€ drivers/
â”‚
â”œâ”€â”€ bronze (schema)
â”‚   â”œâ”€â”€ customers (delta table)
â”‚   â”œâ”€â”€ trips (delta table)
â”‚   â”œâ”€â”€ locations (delta table)
â”‚   â”œâ”€â”€ payments (delta table)
â”‚   â”œâ”€â”€ vehicles (delta table)
â”‚   â”œâ”€â”€ drivers (delta table)
â”‚   â””â”€â”€ checkpoints/ (streaming checkpoints)
â”‚
â”œâ”€â”€ silver (schema)
â”‚   â”œâ”€â”€ customers (delta table - deduplicated + upserted)
â”‚   â”œâ”€â”€ trips (delta table)
â”‚   â”œâ”€â”€ locations (delta table)
â”‚   â”œâ”€â”€ payments (delta table)
â”‚   â”œâ”€â”€ vehicles (delta table)
â”‚   â””â”€â”€ drivers (delta table)
â”‚
â””â”€â”€ gold (schema)
    â”œâ”€â”€ FactTrips (incremental model)
    â”œâ”€â”€ DimCustomers (snapshot - SCD Type 2)
    â”œâ”€â”€ DimDrivers (snapshot - SCD Type 2)
    â”œâ”€â”€ DimVehicles (snapshot - SCD Type 2)
    â”œâ”€â”€ DimLocations (snapshot - SCD Type 2)
    â””â”€â”€ DimPayments (snapshot - SCD Type 2)
```

### Key Entities:
- **customers**: Customer information
- **trips**: Trip details (trip_id, customer_id, driver_id, timestamps, fare, distance)
- **locations**: Pickup/dropoff locations
- **payments**: Payment transactions
- **vehicles**: Vehicle information
- **drivers**: Driver information

---

## ğŸš€ CÃ¡ch thiáº¿t láº­p vÃ  cháº¡y (Setup & Run)

### Prerequisites:
- Databricks workspace (Community Edition hoáº·c Trial)
- Python 3.8+
- dbt-databricks package

### BÆ°á»›c 1: Thiáº¿t láº­p Databricks Cluster
```bash
# Khá»Ÿi táº¡o cluster vá»›i:
- Runtime: DBR 13.3 LTS hoáº·c má»›i hÆ¡n
- Node type: Standard (phÃ¹ há»£p vá»›i community edition)
- Enable Unity Catalog
```

### BÆ°á»›c 2: Upload dá»¯ liá»‡u lÃªn Volumes
```sql
-- Táº¡o catalog vÃ  schema
CREATE CATALOG IF NOT EXISTS pysparkdbt;
CREATE SCHEMA IF NOT EXISTS pysparkdbt.source;
CREATE SCHEMA IF NOT EXISTS pysparkdbt.bronze;
CREATE SCHEMA IF NOT EXISTS pysparkdbt.silver;
CREATE SCHEMA IF NOT EXISTS pysparkdbt.gold;

-- Táº¡o Volume
CREATE VOLUME IF NOT EXISTS pysparkdbt.source.source_data;
```

Upload cÃ¡c file CSV vÃ o `/Volumes/pysparkdbt/source/source_data/{entity}/`

### BÆ°á»›c 3: Cháº¡y Bronze Ingestion
```bash
# Má»Ÿ notebook: databricks/notebooks/bronze_ingestion.ipynb
# Cháº¡y táº¥t cáº£ cells Ä‘á»ƒ ingest dá»¯ liá»‡u tá»« CSV vÃ o Bronze Delta tables
```

**CÃ´ng viá»‡c notebook nÃ y:**
- Äá»c CSV tá»« Volumes vá»›i Spark Streaming
- Infer schema tá»« batch read
- Write stream vá»›i `trigger(once=True)` vÃ o Bronze tables
- LÆ°u checkpoints Ä‘á»ƒ tracking progress

### BÆ°á»›c 4: Cháº¡y Silver Transformation
```bash
# Má»Ÿ notebook: databricks/notebooks/silver_transformation.ipynb
# Cháº¡y táº¥t cáº£ cells Ä‘á»ƒ transform vÃ  upsert vÃ o Silver tables
```

**CÃ´ng viá»‡c notebook nÃ y:**
- **Deduplication**: Sá»­ dá»¥ng `row_number()` window function Ä‘á»ƒ loáº¡i bá» duplicates
- **CDC Processing**: So sÃ¡nh timestamp Ä‘á»ƒ giá»¯ báº£n ghi má»›i nháº¥t
- **Upsert logic**: DÃ¹ng `DeltaTable.merge()` Ä‘á»ƒ update hoáº·c insert
- **Process timestamp**: ThÃªm metadata timestamp cho audit

### BÆ°á»›c 5: CÃ i Ä‘áº·t dbt
```bash
# CÃ i Ä‘áº·t dbt-databricks
pip install dbt-databricks

# Navigate to dbt project
cd dbt_project

# Cáº¥u hÃ¬nh profiles.yml
# Táº¡o file ~/.dbt/profiles.yml vá»›i thÃ´ng tin káº¿t ná»‘i Databricks
```

**Cáº¥u hÃ¬nh profiles.yml máº«u:**
```yaml
default:
  outputs:
    dev:
      type: databricks
      catalog: pysparkdbt
      schema: gold
      host: <your-databricks-workspace-url>
      http_path: <your-cluster-http-path>
      token: <your-access-token>
      threads: 4
  target: dev
```

### BÆ°á»›c 6: Cháº¡y dbt models
```bash
# Test káº¿t ná»‘i
dbt debug

# Cháº¡y incremental models (FactTrips)
dbt run --select silver.trips

# Cháº¡y snapshots Ä‘á»ƒ táº¡o Dimension tables vá»›i SCD Type 2
dbt snapshot
```

**Káº¿t quáº£ dbt snapshot:**
- Táº¡o cÃ¡c báº£ng Dimension vá»›i columns: `dbt_scd_id`, `dbt_updated_at`, `dbt_valid_from`, `dbt_valid_to`
- Track lá»‹ch sá»­ thay Ä‘á»•i cá»§a tá»«ng record
- `dbt_valid_to = '9999-12-31'` cho current records

### BÆ°á»›c 7: Kiá»ƒm tra dá»¯ liá»‡u
```sql
-- Kiá»ƒm tra FactTrips
SELECT * FROM pysparkdbt.gold.trips LIMIT 10;

-- Kiá»ƒm tra DimCustomers (SCD Type 2)
SELECT * FROM pysparkdbt.gold.dimcustomers 
WHERE customer_id = 'C001'
ORDER BY dbt_valid_from;

-- Kiá»ƒm tra sá»‘ lÆ°á»£ng records
SELECT 
  'Bronze' as layer, COUNT(*) as row_count FROM pysparkdbt.bronze.trips
UNION ALL
SELECT 
  'Silver' as layer, COUNT(*) as row_count FROM pysparkdbt.silver.trips
UNION ALL
SELECT 
  'Gold' as layer, COUNT(*) as row_count FROM pysparkdbt.gold.trips;
```

---

## ğŸ“ˆ Káº¿t quáº£ (Final Output)

### Gold Layer Tables:

**1. FactTrips (Incremental Model)**
- Chá»©a toÃ n bá»™ thÃ´ng tin trips vá»›i incremental load
- Chá»‰ load records má»›i dá»±a trÃªn `last_updated_timestamp`

**2. Dimension Tables (SCD Type 2)**
- **DimCustomers**: Lá»‹ch sá»­ thay Ä‘á»•i thÃ´ng tin customers
- **DimDrivers**: Lá»‹ch sá»­ thay Ä‘á»•i thÃ´ng tin drivers
- **DimVehicles**: Lá»‹ch sá»­ thay Ä‘á»•i thÃ´ng tin vehicles
- **DimLocations**: Lá»‹ch sá»­ thay Ä‘á»•i thÃ´ng tin locations
- **DimPayments**: Lá»‹ch sá»­ thay Ä‘á»•i thÃ´ng tin payments

### Sample Query Results:
```sql
-- PhÃ¢n tÃ­ch trips theo driver vá»›i historical data
SELECT 
  d.driver_id,
  d.driver_name,
  d.dbt_valid_from,
  d.dbt_valid_to,
  COUNT(t.trip_id) as total_trips,
  SUM(t.fare_amount) as total_revenue
FROM pysparkdbt.gold.trips t
JOIN pysparkdbt.gold.dimdriver d 
  ON t.driver_id = d.driver_id
  AND t.trip_start_time BETWEEN d.dbt_valid_from AND d.dbt_valid_to
GROUP BY 1, 2, 3, 4
ORDER BY total_revenue DESC;
```

---

## ğŸ“ Cáº¥u trÃºc Project

```
uber-databricks-dbt-pipeline/
â”œâ”€â”€ databricks/
â”‚   â””â”€â”€ notebooks/
â”‚       â”œâ”€â”€ bronze_ingestion.ipynb      # PySpark Streaming ingestion
â”‚       â””â”€â”€ silver_transformation.ipynb # Dedup + Upsert logic
â”œâ”€â”€ dbt_project/
â”‚   â”œâ”€â”€ dbt_project.yml                 # dbt configuration
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â”‚   â””â”€â”€ trips.sql               # Incremental model
â”‚   â”‚   â””â”€â”€ sources/
â”‚   â”‚       â””â”€â”€ sources.yaml            # Source definitions
â”‚   â”œâ”€â”€ snapshots/
â”‚   â”‚   â”œâ”€â”€ SCDs.yaml                   # Dimension snapshots (SCD Type 2)
â”‚   â”‚   â””â”€â”€ snap_fact.yaml
â”‚   â”œâ”€â”€ macros/
â”‚   â”‚   â””â”€â”€ generate_schema_name.sql    # Custom schema macro
â”‚   â”œâ”€â”€ tests/                          # Custom data tests
â”‚   â””â”€â”€ analyses/
â”‚       â””â”€â”€ scratch.sql                 # Ad-hoc queries
â”œâ”€â”€ requirements.txt                    # Python dependencies
â””â”€â”€ README.md                           # This file
```

---

## ğŸ¯ Key Features

âœ… **Medallion Architecture**: Bronze â†’ Silver â†’ Gold layers vá»›i Delta Lake  
âœ… **Incremental Processing**: Chá»‰ xá»­ lÃ½ dá»¯ liá»‡u má»›i, tiáº¿t kiá»‡m compute  
âœ… **Data Quality**: Deduplication vÃ  upsert logic vá»›i CDC  
âœ… **Historical Tracking**: SCD Type 2 cho Dimension tables  
âœ… **Scalability**: Spark Streaming vÃ  Delta Lake optimization  
âœ… **Idempotency**: Checkpoints vÃ  merge operations Ä‘áº£m báº£o re-run safety  

